{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegNet implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution layer. takes parameters; input tensor, number of filters and kernel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(input, filters, kernel_size = 7):\n",
    "    return tf.layers.conv2d(\n",
    "            inputs = input,\n",
    "            filters = filters,\n",
    "            kernel_size = [kernel_size, kernel_size],\n",
    "            padding = \"same\",\n",
    "            activation = tf.nn.relu\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution layer. takes parameters; input tensor, number of filters. Same as the Convolution layer except that it neglects the biases and does not apply activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deconv_layer(input,filters):\n",
    "    return tf.layers.conv2d(\n",
    "            inputs = input,\n",
    "            filters = filters,\n",
    "            kernel_size = [7,7],\n",
    "            padding = \"same\",\n",
    "            use_bias=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pool layer which takes input tensor and returns a tensor with half size and map of the max pooled tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool_layer(input):\n",
    "    return(tf.nn.max_pool_with_argmax(input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unpool layer which takes an input tensor and the map of indices and returns the unpooled tensor of double size of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpool_layer(input, index, ksize=[1, 2, 2, 1]):\n",
    "    input_shape = input.get_shape().as_list()\n",
    "    output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n",
    "\n",
    "    flat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\n",
    "\n",
    "    uppool = tf.reshape(input, [np.prod(input_shape)])\n",
    "    batch_range = tf.reshape(tf.range(output_shape[0], dtype=index.dtype), shape=[input_shape[0], 1, 1, 1])\n",
    "    b = tf.ones_like(index) * batch_range\n",
    "    b = tf.reshape(b, [np.prod(input_shape), 1])\n",
    "    ind = tf.reshape(index, [np.prod(input_shape), 1])\n",
    "    ind = tf.concat([b, ind], 1)\n",
    "\n",
    "    output = tf.scatter_nd(ind, uppool, shape=flat_output_shape)\n",
    "    return tf.reshape(output, output_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main model of Segnet which consists of 13 Convolution layer, 5 max pooling layers, 13 deconvloution layers and 5 unpooling layers. Attached to pixelwise classifier. \n",
    "We assumed that we only have two classes.\n",
    "Still missing the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input, scope=\"SegNet\", reuse=True):\n",
    "    input = tf.reshape(input[\"x\"], [-1, 256, 256, 1])\n",
    "    n_labels = 2\n",
    "    with tf.name_scope(scope):\n",
    "        # first layer\n",
    "        conv_1 = conv_layer(input, 64)\n",
    "        conv_2 = conv_layer(conv_1, 64)\n",
    "        pool_1, indicies_1 = maxpool_layer(conv_2)\n",
    "        \n",
    "        # second layer\n",
    "        conv_3 = conv_layer(pool_1, 128)\n",
    "        conv_4 = conv_layer(conv_3, 128)\n",
    "        pool_2, indicies_2 = maxpool_layer(conv_4)\n",
    "        \n",
    "        #third layer\n",
    "        conv_5 = conv_layer(pool_2, 256)\n",
    "        conv_6 = conv_layer(conv_5, 256)\n",
    "        conv_7 = conv_layer(conv_6, 256)\n",
    "        pool_3, indicies_3 = maxpool_layer(conv_7)\n",
    "        \n",
    "        #fourth layer\n",
    "        conv_8 = conv_layer(pool_3, 512)\n",
    "        conv_9 = conv_layer(conv_8, 512)\n",
    "        conv_10 = conv_layer(conv_9, 512)\n",
    "        pool_4, indicies_4 = maxpool_layer(conv_10)\n",
    "        \n",
    "        #fifth layer\n",
    "        conv_11 = conv_layer(pool_4, 512)\n",
    "        conv_12 = conv_layer(conv_11, 512)\n",
    "        conv_13 = conv_layer(conv_12, 512)\n",
    "        pool_5, indicies_5 = maxpool_layer(conv_13)\n",
    "        \n",
    "        \n",
    "        # the decoding layers\n",
    "        # fifth layer\n",
    "        unpool_5 = unpool_layer(pool_5, indicies_5)\n",
    "        deconv_13 = deconv_layer(unpool_5, 512)\n",
    "        deconv_12 = deconv_layer(deconv_13, 512)\n",
    "        deconv_11 = deconv_layer(deconv_12, 512)\n",
    "       \n",
    "        \n",
    "        # forth layer\n",
    "        unpool_4 = unpool_layer(deconv_11, indicies_4)\n",
    "        deconv_10 = deconv_layer(unpool_4, 512)\n",
    "        deconv_9 = deconv_layer(deconv_10, 512)\n",
    "        deconv_8 = deconv_layer(deconv_9, 256)\n",
    "        \n",
    "        \n",
    "        # third layer\n",
    "        unpool_3 = unpool_layer(deconv_8, indicies_3)\n",
    "        deconv_7 = deconv_layer(unpool_3, 256)\n",
    "        deconv_6 = deconv_layer(deconv_7, 256)\n",
    "        deconv_5 = deconv_layer(deconv_6, 128)\n",
    "        \n",
    "        # second layer\n",
    "        unpool_2 = unpool_layer(deconv_5, indicies_2)\n",
    "        deconv_4 = deconv_layer(unpool_2, 128)\n",
    "        deconv_3 = deconv_layer(deconv_4, 64)\n",
    "        \n",
    "        # first layer\n",
    "        unpool_1 = unpool_layer(deconv_3, indicies_1)\n",
    "        deconv_2 = deconv_layer(unpool_1, 64)\n",
    "        #deconv_1 = deconv_layer(deconv_2, 64)\n",
    "        \n",
    "        # Classification\n",
    "        classifier_layer = conv_layer(deconv_2, n_labels, kernel_size = 1)\n",
    "        softmax = tf.nn.softmax(classifier_layer)\n",
    "        \n",
    "        onehot_labels = tf.one_hot(indices= inputs[\"y\"], depth = 2)\n",
    "        loss = tf.losses.softmax_cross_entropy(onehot_labels = onehot_labels, logits = classifier_layer)\n",
    "        print(loss)\n",
    "        print(softmax.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that the model is working properly with the liver input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (1, 256, 256, 2) and (256, 256, 2) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-d5dd3ee3ee7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-f9d37e5f762e>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(input, scope, reuse)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0monehot_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monehot_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy\u001b[0;34m(onehot_labels, logits, weights, label_smoothing, scope, loss_collection, reduction)\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0monehot_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel_smoothing\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \"\"\"\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (1, 256, 256, 2) and (256, 256, 2) are incompatible"
     ]
    }
   ],
   "source": [
    "im = cv2.imread('liver.jpeg', cv2.IMREAD_GRAYSCALE)\n",
    "im = cv2.resize(im,(256,256))\n",
    "im = np.array(im, dtype=np.float32)\n",
    "im = np.expand_dims(im, axis = 0)\n",
    "im = np.expand_dims(im, axis = 3)\n",
    "labels = [[random.randint(0,1) for i in range(256)] for j in range(256)]\n",
    "labels = np.array(labels)\n",
    "prizzz\n",
    "inputs = {\"x\": im, \"y\": labels}\n",
    "model(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
